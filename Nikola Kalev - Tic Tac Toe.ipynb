{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Learning Tic-Tac-Toe Game by Nikola Kalev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tik Tak Toe Game is relatively simple game. The game consists of total 26,830 unique game states. However, I'm trying to train NN (Neural Networ) how to win, block and and strategize effectively within the game.\n",
    "\n",
    "#### As my objective is to iterate over a large number of games, I'm applying Q-Learning Technique which is part of the Reinforcment Learning\n",
    "\n",
    "### Structure of the Notebook:\n",
    "1. **Build a simple version of Tic Tak Toe Game - 3 X 3 (no visual imputs)**\n",
    "    - add small helper functions\n",
    "\n",
    "2. **Build Neural Net with Tensorflow and Keras**\n",
    "\n",
    "3. **Buld Q-Learning Class**\n",
    "\n",
    "4. **Train the agent to play against itself**\n",
    "\n",
    "5. **Observe the results from the training**\n",
    "\n",
    "6. **Build function so human can test and play against the trained agent**\n",
    "\n",
    "7. **Next steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Simple Tic-Tak-Toe Game as class object in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [' ']*9\n",
    "        self.player = 'O'\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [i for i, x in enumerate(self.board) if x == ' ']\n",
    "\n",
    "    def make_move(self, position, player):\n",
    "        if self.board[position] == ' ':\n",
    "            self.board[position] = player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_win(self):\n",
    "        win_conditions = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        for condition in win_conditions:\n",
    "            if self.board[condition[0]] == self.board[condition[1]] == self.board[condition[2]] != ' ':\n",
    "                return True\n",
    "        if ' ' not in self.board:\n",
    "            return True  # the game is over if the board is full\n",
    "        return False\n",
    "\n",
    "    def is_about_to_win(self, player):\n",
    "        board = np.array(self.board).reshape((3, 3))\n",
    "\n",
    "        # Check rows\n",
    "        for row in board:\n",
    "            if (row == player).sum() == 2 and (row == ' ').sum() == 1:\n",
    "                return True\n",
    "\n",
    "        # Check columns\n",
    "        for col in board.T:  # .T transposes the board to treat columns as rows\n",
    "            if (col == player).sum() == 2 and (col == ' ').sum() == 1:\n",
    "                return True\n",
    "\n",
    "        # Check diagonals\n",
    "        diag1 = np.array([board[i, i] for i in range(3)])\n",
    "        diag2 = np.array([board[i, 2 - i] for i in range(3)])\n",
    "        if ((diag1 == player).sum() == 2 and (diag1 == ' ').sum() == 1) or \\\n",
    "        ((diag2 == player).sum() == 2 and (diag2 == ' ').sum() == 1):\n",
    "            return True\n",
    "\n",
    "        # If no two-in-a-row situation was found, return False\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following function transforms the board, allowing it to be read and understand by the Neural Network (NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(board):\n",
    "    transform = {'X': [1, 0, 0], 'O': [0, 1, 0], ' ': [0, 0, 1]}\n",
    "    return np.array([transform[i] for i in board]).reshape(1, 9, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a Feed-forward Neural Network with a Dropout layer of 20%, using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, 9, 3)))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(9))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.005)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Reinforcement Learning process using a Q-Learning Agent. The main objective is to enable the agent to learn by interacting with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, model, epsilon=0.1, discount_factor=0.7):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.memory = []\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(available_actions)\n",
    "        else:\n",
    "            q_values = self.model.predict(state)[0]\n",
    "            q_values = {action: q_values[action] for action in available_actions}\n",
    "            return max(q_values, key=q_values.get)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((one_hot_encode(state).reshape(1, 9, 3), action, reward, one_hot_encode(next_state).reshape(1, 9, 3), done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "\n",
    "        # Prepare batch data\n",
    "        states_batch = np.array([np.array(experience[0]).reshape(1, 9, 3) for experience in minibatch])\n",
    "        next_states_batch = np.array([np.array(experience[3]).reshape(1, 9, 3) for experience in minibatch])\n",
    "        targets_batch = np.zeros((states_batch.shape[0], 9))  # Assuming 9 possible actions\n",
    "\n",
    "        # Compute targets\n",
    "        current_qs = self.model.predict(states_batch)\n",
    "        future_qs = self.model.predict(next_states_batch)\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor * np.max(future_qs[i])\n",
    "            current_qs[i][action] = target\n",
    "            targets_batch[i] = current_qs[i]\n",
    "\n",
    "        # Perform batch training\n",
    "        self.model.fit(states_batch, targets_batch, epochs=1, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent - 1000 Games. Instead of having the agent play against random choices on the board, we set up an Agent vs Agent environment. This allows the agents to learn from each other moves and build strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent1, agent2, games=1000, batch_size=32):\n",
    "    for _ in range(games):\n",
    "        game = TicTacToe()\n",
    "        current_player = 'X'\n",
    "        while not game.check_win():\n",
    "            # The current agent takes its chosen action\n",
    "            state = game.board.copy()\n",
    "            available_moves = game.available_moves()\n",
    "            if not available_moves: break  # the game is over\n",
    "            \n",
    "            if current_player == 'X':\n",
    "                action = agent1.choose_action(one_hot_encode(state), available_moves)\n",
    "            else:\n",
    "                action = agent2.choose_action(one_hot_encode(state), available_moves)\n",
    "                \n",
    "            game.make_move(action, current_player)\n",
    "\n",
    "            # Remember the state, action, reward, and next state\n",
    "            next_state = game.board.copy()\n",
    "            if game.check_win():\n",
    "                if current_player == 'X':\n",
    "                    reward = 1  # agent1 won\n",
    "                    agent1.remember(state, action, reward, next_state, game.check_win())\n",
    "                    agent2.remember(state, action, -reward, next_state, game.check_win())  # negative reward for losing\n",
    "                else:\n",
    "                    reward = 1  # agent2 won\n",
    "                    agent2.remember(state, action, reward, next_state, game.check_win())\n",
    "                    agent1.remember(state, action, -reward, next_state, game.check_win())  # negative reward for losing\n",
    "            else:\n",
    "                if game.is_about_to_win('X') and not game.is_about_to_win('O'):\n",
    "                    reward = 2  # agent successfully blocked opponent\n",
    "                    agent1.remember(state, action, reward, next_state, game.check_win())\n",
    "                    agent2.remember(state, action, -reward, next_state, game.check_win())  # negative reward for not blocking\n",
    "                elif game.is_about_to_win('O') and not game.is_about_to_win('X'):  # opponent is about to win, but agent didn't block\n",
    "                    reward = -2.0  # heavily penalize the agent for not blocking\n",
    "                    agent1.remember(state, action, reward, next_state, game.check_win())\n",
    "                    agent2.remember(state, action, reward, next_state, game.check_win())\n",
    "                else:\n",
    "                    reward = -0.1  # encourage the agent to win quickly\n",
    "                    agent1.remember(state, action, reward, next_state, game.check_win())\n",
    "                    agent2.remember(state, action, reward, next_state, game.check_win())\n",
    "\n",
    "            # Replay past experiences to learn\n",
    "            agent1.replay(batch_size)\n",
    "            agent2.replay(batch_size)\n",
    "            \n",
    "            # Swap players\n",
    "            current_player = 'O' if current_player == 'X' else 'X'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the agents and store the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_agents(agent1, agent2, games=100):\n",
    "    results = {'Agent1': {'Win': 0, 'Lose': 0, 'Draw': 0},\n",
    "               'Agent2': {'Win': 0, 'Lose': 0, 'Draw': 0}}\n",
    "\n",
    "    for _ in range(games):\n",
    "        game = TicTacToe()\n",
    "        current_player = 'X'\n",
    "        while not game.check_win():\n",
    "            # The current agent takes its chosen action\n",
    "            state = game.board.copy()\n",
    "            available_moves = game.available_moves()\n",
    "            if not available_moves: break  # the game is over\n",
    "\n",
    "            if current_player == 'X':\n",
    "                action = agent1.choose_action(one_hot_encode(state), available_moves)\n",
    "            else:\n",
    "                action = agent2.choose_action(one_hot_encode(state), available_moves)\n",
    "\n",
    "            game.make_move(action, current_player)\n",
    "\n",
    "            if game.check_win():\n",
    "                if current_player == 'X':\n",
    "                    results['Agent1']['Win'] += 1\n",
    "                    results['Agent2']['Lose'] += 1\n",
    "                else:\n",
    "                    results['Agent2']['Win'] += 1\n",
    "                    results['Agent1']['Lose'] += 1\n",
    "            elif ' ' not in game.board:  # the game is a draw\n",
    "                results['Agent1']['Draw'] += 1\n",
    "                results['Agent2']['Draw'] += 1\n",
    "\n",
    "            # Swap players\n",
    "            current_player = 'O' if current_player == 'X' else 'X'\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_model()\n",
    "agent1 = QLearningAgent(model1)\n",
    "\n",
    "model2 = build_model()\n",
    "agent2 = QLearningAgent(model2)\n",
    "\n",
    "train_agent(agent1, agent2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the results from the trained agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_agents(agent1, agent2, games = 100)\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Human Playgroud and Load the Trained agents:\n",
    "Given that the Tic-Tac-Toe game favors the player who moves first, and considering the limited time I have for training the agent over a larger number of games, the code below gives the advantage to the agent (NN) by having it play first.\n",
    "\n",
    "This could be changed once the agent is better trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_model()\n",
    "agent1 = QLearningAgent(load_model(\"agent1_model_02062023.h5\"))\n",
    "\n",
    "model2 = build_model()\n",
    "agent2 = QLearningAgent(load_model(\"agent2_model_02062023.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(board):\n",
    "    print(\"-------------\")\n",
    "    for i in range(0, 9, 3):\n",
    "        print(\"|\", board[i], \"|\", board[i+1], \"|\", board[i+2], \"|\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "\n",
    "## Agent play First:\n",
    "def play_game(agent):\n",
    "    game = TicTacToe()\n",
    "    while not game.check_win():\n",
    "        # The agent (player X) takes its chosen action\n",
    "        state = game.board.copy()\n",
    "        available_moves = game.available_moves()\n",
    "        if available_moves:\n",
    "            action = agent.choose_action(one_hot_encode(state), available_moves)\n",
    "            game.make_move(action, 'X')\n",
    "            if game.check_win():\n",
    "                print('The agent won.')\n",
    "                break\n",
    "\n",
    "        # Player O takes a move based on user input\n",
    "        print('Current board:')\n",
    "        print_board(game.board)\n",
    "        available_moves = game.available_moves()\n",
    "        if not available_moves:\n",
    "            print('It is a draw.')\n",
    "            break\n",
    "        print('Available moves:', available_moves)\n",
    "        move = int(input('Your move (0-8): '))\n",
    "        while move not in available_moves:\n",
    "            print('Invalid move.')\n",
    "            move = int(input('Your move (0-8): '))\n",
    "        game.make_move(move, 'O')\n",
    "        if game.check_win():\n",
    "            print('You won!')\n",
    "            break\n",
    "\n",
    "## Human play First:\n",
    "# def play_game(agent):\n",
    "#     game = TicTacToe()\n",
    "#     while not game.check_win():\n",
    "#         # Player X takes a move based on user input\n",
    "#         print('Current board:')\n",
    "#         print_board(game.board)\n",
    "#         available_moves = game.available_moves()\n",
    "#         print('Available moves:', available_moves)\n",
    "#         move = int(input('Your move (0-8): '))\n",
    "#         while move not in available_moves:\n",
    "#             print('Invalid move.')\n",
    "#             move = int(input('Your move (0-8): '))\n",
    "#         game.make_move(move, 'X')\n",
    "#         if game.check_win():\n",
    "#             print('You won!')\n",
    "#             break\n",
    "\n",
    "#         # The agent (player O) takes its chosen action\n",
    "#         state = game.board.copy()\n",
    "#         available_moves = game.available_moves()\n",
    "#         if available_moves:\n",
    "#             action = agent.choose_action(one_hot_encode(state), available_moves)\n",
    "#             game.make_move(action, 'O')\n",
    "#             if game.check_win():\n",
    "#                 print('The agent won.')\n",
    "#                 break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Play the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(agent1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps:\n",
    "1. Train the agent over a much larger sample of games.\n",
    "\n",
    "2. Make modifications to the Neural Network, such as:\n",
    "    - Change the hyperparameters\n",
    "    - Making it deeper and adding more Dropout layers\n",
    "    - Changing the optimizers\n",
    "    \n",
    "\n",
    "3. Further exploration why there is no draw games in the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf1ead898c8ad247136065ca735c9a8fd5091140354bef2469f302996cc681e3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
